{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zlw1225/MPLC_CUDA/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf2QOKH6zwJj",
        "outputId": "fa2c0ac6-44d7-4bbd-9852-9bb3389f292b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "git version 2.34.1\n"
          ]
        }
      ],
      "source": [
        "!git --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSqwNWfxz44v",
        "outputId": "4339ad50-2269-40b1-d4dd-872eaa4d59e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MPLC_CUDA'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 9 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (9/9), 17.57 MiB | 12.41 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zlw1225/MPLC_CUDA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xndSOjgqzJcq",
        "outputId": "e4d9588c-7575-42bf-e262-6a9499c7c7d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/MPLC_CUDA'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "path=\"/content/MPLC_CUDA\"\n",
        "os.chdir(path)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8irrMHU1NF_",
        "outputId": "3504323c-f4b9-447b-a96e-e5250873d3ca"
      },
      "outputs": [],
      "source": [
        "!python MPLC_CUDA2.py --iterations 2 --calc_perf_every_it 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "D:/ZLW/project/MPLC_CUDA/.venv/Scripts/python.exe MPLC_CUDA2.py --iterations 2 --calc_perf_every_it 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "function ClickConnect() {\n",
        "  var connectButton = document.querySelector(\"colab-toolbar-button#connect\");\n",
        "  if(connectButton != null) {\n",
        "    console.log(\"Working\"); \n",
        "    connectButton.click();\n",
        "  }\n",
        "}\n",
        "setInterval(ClickConnect, 60000);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--n_of_modes N_OF_MODES] [--Planes PLANES]\n",
            "                             [--iterations ITERATIONS]\n",
            "                             [--first_n_iterations FIRST_N_ITERATIONS]\n",
            "                             [--Nx NX] [--Ny NY]\n",
            "                             [--calc_perf_every_it CALC_PERF_EVERY_IT]\n",
            "                             [--equalize_efficiency {0,1}]\n",
            "                             [--plot_eff_distribution {0,1}]\n",
            "                             [--smoothing_switch {0,1}] [--plot_results {0,1}]\n",
            "                             [--do_padded_eval {0,1}] [--alpha ALPHA]\n",
            "                             [--beta BETA] [--gamma GAMMA]\n",
            "                             [--delta_theta_1 DELTA_THETA_1]\n",
            "                             [--delta_theta_0 DELTA_THETA_0]\n",
            "                             [--pixelSize PIXELSIZE] [--wavelength WAVELENGTH]\n",
            "                             [--d_in D_IN] [--d D] [--d_out D_OUT]\n",
            "                             [--OffsetMultiplier OFFSETMULTIPLIER]\n",
            "ipykernel_launcher.py: error: argument --first_n_iterations: invalid int value: 'c:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v39fc61972716c845a3b5a99dde57cc3519e743747.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MPLC2] Using device: cuda\n",
            "[MPLC2] GPU: NVIDIA GeForce RTX 3060, capability=(8, 6), torch_cuda=12.1\n",
            "iteration 10 : loc. fidelity = 92.41 , crosstalk = 7.25 , efficiency = 30.59\n",
            "iteration 20 : loc. fidelity = 92.95 , crosstalk = 3.87 , efficiency = 31.63\n",
            "iteration 30 : loc. fidelity = 93.11 , crosstalk = 3.42 , efficiency = 36.21\n",
            "iteration 40 : loc. fidelity = 93.2 , crosstalk = 3.14 , efficiency = 37.96\n",
            "iteration 50 : loc. fidelity = 93.28 , crosstalk = 2.99 , efficiency = 40.09\n",
            "iteration 60 : loc. fidelity = 93.33 , crosstalk = 2.82 , efficiency = 41.54\n",
            "iteration 70 : loc. fidelity = 93.37 , crosstalk = 2.74 , efficiency = 43.06\n",
            "iteration 80 : loc. fidelity = 93.4 , crosstalk = 2.65 , efficiency = 44.25\n",
            "iteration 90 : loc. fidelity = 93.43 , crosstalk = 2.6 , efficiency = 45.35\n",
            "iteration 100 : loc. fidelity = 93.45 , crosstalk = 2.54 , efficiency = 46.26\n",
            "iteration 110 : loc. fidelity = 93.46 , crosstalk = 2.5 , efficiency = 47.08\n",
            "iteration 120 : loc. fidelity = 93.47 , crosstalk = 2.46 , efficiency = 47.78\n",
            "iteration 130 : loc. fidelity = 93.49 , crosstalk = 2.42 , efficiency = 48.4\n",
            "iteration 140 : loc. fidelity = 93.5 , crosstalk = 2.37 , efficiency = 48.93\n",
            "iteration 150 : loc. fidelity = 93.51 , crosstalk = 2.33 , efficiency = 49.4\n",
            "iteration 160 : loc. fidelity = 93.52 , crosstalk = 2.29 , efficiency = 49.81\n",
            "iteration 170 : loc. fidelity = 93.53 , crosstalk = 2.25 , efficiency = 50.18\n",
            "iteration 180 : loc. fidelity = 93.54 , crosstalk = 2.21 , efficiency = 50.51\n",
            "iteration 190 : loc. fidelity = 93.55 , crosstalk = 2.19 , efficiency = 50.8\n",
            "iteration 200 : loc. fidelity = 93.56 , crosstalk = 2.15 , efficiency = 51.07\n",
            "iteration 210 : loc. fidelity = 93.57 , crosstalk = 2.12 , efficiency = 51.3\n",
            "iteration 220 : loc. fidelity = 93.58 , crosstalk = 2.1 , efficiency = 51.51\n",
            "iteration 230 : loc. fidelity = 93.59 , crosstalk = 2.07 , efficiency = 51.7\n",
            "iteration 240 : loc. fidelity = 93.6 , crosstalk = 2.04 , efficiency = 51.87\n",
            "iteration 250 : loc. fidelity = 93.61 , crosstalk = 2.02 , efficiency = 52.02\n",
            "iteration 260 : loc. fidelity = 93.62 , crosstalk = 2.0 , efficiency = 52.17\n",
            "iteration 270 : loc. fidelity = 93.63 , crosstalk = 1.98 , efficiency = 52.29\n",
            "iteration 280 : loc. fidelity = 93.63 , crosstalk = 1.96 , efficiency = 52.4\n",
            "iteration 290 : loc. fidelity = 93.64 , crosstalk = 1.95 , efficiency = 52.51\n",
            "iteration 300 : loc. fidelity = 93.65 , crosstalk = 1.93 , efficiency = 52.6\n",
            "Final performance (avg over λ): loc. fidelity = 93.649 , crosstalk = 1.932 , efficiency = 52.601\n",
            "λ=1.530 µm -> fidelity=92.702, crosstalk=3.115, efficiency=45.908\n",
            "λ=1.550 µm -> fidelity=93.863, crosstalk=1.805, efficiency=53.171\n",
            "λ=1.570 µm -> fidelity=94.265, crosstalk=1.357, efficiency=56.807\n",
            "λ=1.590 µm -> fidelity=94.245, crosstalk=1.306, efficiency=57.160\n",
            "λ=1.610 µm -> fidelity=93.806, crosstalk=1.619, efficiency=54.060\n",
            "λ=1.625 µm -> fidelity=93.017, crosstalk=2.390, efficiency=48.503\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5960\\1029996445.py:509: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n",
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5960\\1029996445.py:588: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wavelengths (μm): ['1.530', '1.550', '1.570', '1.590', '1.610', '1.625']\n",
            "IL (dB):          ['-3.034', '-2.414', '-2.141', '-2.130', '-2.395', '-2.878']\n",
            "MDL (dB):         ['2.030', '1.393', '1.159', '1.260', '1.579', '2.102']\n",
            "XTs_avg (dB):     ['-18.640', '-21.652', '-23.617', '-23.502', '-21.299', '-18.931']\n",
            "fidelity:         ['92.702', '93.863', '94.265', '94.245', '93.806', '93.017']\n",
            "crosstalk:        ['3.115', '1.805', '1.357', '1.306', '1.619', '2.390']\n",
            "efficiency:       ['45.908', '53.171', '56.807', '57.160', '54.060', '48.503']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5960\\1029996445.py:638: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "# custom functions imported from the utils.py file available within the package\n",
        "from utils import *\n",
        "\n",
        "DEFAULTS = {\n",
        "    \"n_of_modes\": 10,\n",
        "    \"Planes\": 9,\n",
        "    \"iterations\": 300,\n",
        "    # objective weights\n",
        "    \"alpha\": 1.0,\n",
        "    \"beta\": 2.0,\n",
        "    \"gamma\": 0.0,\n",
        "    # optimization schedule\n",
        "    \"first_n_iterations\": 10,\n",
        "    \"delta_theta_1\": 2*math.pi/255,  # usual step size\n",
        "    \"delta_theta_0\": 10*(2*math.pi/255),  # bigger step size (default 10x)\n",
        "    # sampling / optics\n",
        "    \"Nx\": 512,\n",
        "    \"Ny\": 512,\n",
        "    \"pixelSize\": 8e-6,\n",
        "    \"wavelength\": 1.57e-6,\n",
        "    # propagation distances\n",
        "    \"d_in\": 20e-3,\n",
        "    \"d\": 2*9.7e-3,\n",
        "    \"d_out\": 15e-3,\n",
        "    # evaluation cadence / early stop scale\n",
        "    \"calc_perf_every_it\": 10,\n",
        "    # features\n",
        "    \"equalize_efficiency\": 1,\n",
        "    \"plot_eff_distribution\": 0,\n",
        "    \"smoothing_switch\": 1,\n",
        "    # smoothing strength\n",
        "    \"OffsetMultiplier\": 0e-5,\n",
        "    # extras\n",
        "    \"plot_results\": 0,\n",
        "    \"do_padded_eval\": 0,\n",
        "    # acceleration\n",
        "    \"use_amp\": 0,  # optional mixed precision (CUDA only); safe with complex ops (no casting for complex)\n",
        "}\n",
        "\n",
        "def parse_cfg() -> dict:\n",
        "    parser = argparse.ArgumentParser(add_help=True)\n",
        "    # ints\n",
        "    parser.add_argument(\"--n_of_modes\", type=int, default=None)\n",
        "    parser.add_argument(\"--Planes\", type=int, default=None)\n",
        "    parser.add_argument(\"--iterations\", type=int, default=None)\n",
        "    parser.add_argument(\"--first_n_iterations\", type=int, default=None)\n",
        "    parser.add_argument(\"--Nx\", type=int, default=None)\n",
        "    parser.add_argument(\"--Ny\", type=int, default=None)\n",
        "    parser.add_argument(\"--calc_perf_every_it\", type=int, default=None)\n",
        "    parser.add_argument(\"--equalize_efficiency\", type=int, choices=[0,1], default=None)\n",
        "    parser.add_argument(\"--plot_eff_distribution\", type=int, choices=[0,1], default=None)\n",
        "    parser.add_argument(\"--smoothing_switch\", type=int, choices=[0,1], default=None)\n",
        "    parser.add_argument(\"--plot_results\", type=int, choices=[0,1], default=None)\n",
        "    parser.add_argument(\"--do_padded_eval\", type=int, choices=[0,1], default=None)\n",
        "    parser.add_argument(\"--use_amp\", type=int, choices=[0,1], default=None)\n",
        "    # floats\n",
        "    parser.add_argument(\"--alpha\", type=float, default=None)\n",
        "    parser.add_argument(\"--beta\", type=float, default=None)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
        "    parser.add_argument(\"--delta_theta_1\", type=float, default=None)\n",
        "    parser.add_argument(\"--delta_theta_0\", type=float, default=None)\n",
        "    parser.add_argument(\"--pixelSize\", type=float, default=None)\n",
        "    parser.add_argument(\"--wavelength\", type=float, default=None)\n",
        "    parser.add_argument(\"--d_in\", type=float, default=None)\n",
        "    parser.add_argument(\"--d\", type=float, default=None)\n",
        "    parser.add_argument(\"--d_out\", type=float, default=None)\n",
        "    parser.add_argument(\"--OffsetMultiplier\", type=float, default=None)\n",
        "\n",
        "    try:\n",
        "        args = parser.parse_args()\n",
        "    except SystemExit:\n",
        "        # in notebooks or if imported, ignore CLI parsing side-effect\n",
        "        args = argparse.Namespace()\n",
        "    cfg = DEFAULTS.copy()\n",
        "    for k, v in vars(args).items() if hasattr(args, \"__dict__\") else []:\n",
        "        if v is not None:\n",
        "            cfg[k] = v\n",
        "    return cfg\n",
        "\n",
        "CFG = parse_cfg()\n",
        "\n",
        "# concise explicit unpacking (friendly to linters and readers)\n",
        "(n_of_modes, Planes, iterations,\n",
        " alpha, beta, gamma,\n",
        " first_n_iterations, delta_theta_1, delta_theta_0,\n",
        " Nx, Ny, pixelSize, wavelength,\n",
        " d_in, d, d_out,\n",
        " calc_perf_every_it,\n",
        " equalize_efficiency, plot_eff_distribution, smoothing_switch, OffsetMultiplier) = (\n",
        "     CFG[\"n_of_modes\"], CFG[\"Planes\"], CFG[\"iterations\"],\n",
        "     CFG[\"alpha\"], CFG[\"beta\"], CFG[\"gamma\"],\n",
        "     CFG[\"first_n_iterations\"], CFG[\"delta_theta_1\"], CFG[\"delta_theta_0\"],\n",
        "     CFG[\"Nx\"], CFG[\"Ny\"], CFG[\"pixelSize\"], CFG[\"wavelength\"],\n",
        "     CFG[\"d_in\"], CFG[\"d\"], CFG[\"d_out\"],\n",
        "     CFG[\"calc_perf_every_it\"],\n",
        "     CFG[\"equalize_efficiency\"], CFG[\"plot_eff_distribution\"], CFG[\"smoothing_switch\"], CFG[\"OffsetMultiplier\"])\n",
        "\n",
        "# Select device (prefer CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[MPLC2] Using device: {DEVICE}\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    try:\n",
        "        name = torch.cuda.get_device_name(0)\n",
        "        cap = torch.cuda.get_device_capability(0)\n",
        "        print(f\"[MPLC2] GPU: {name}, capability={cap}, torch_cuda={getattr(torch.version, 'cuda', None)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[MPLC2] CUDA detected but failed to query device info: {e}\")\n",
        "else:\n",
        "    print(\"[MPLC2] CUDA 不可用：将使用 CPU 运行。若期望使用 GPU，请安装 CUDA 版 PyTorch 并确保驱动正确。\")\n",
        "\n",
        "# optional AMP context (only meaningful on CUDA; complex ops keep their dtype)\n",
        "from contextlib import nullcontext\n",
        "def autocast_if_cuda():\n",
        "    use_amp = bool(CFG.get(\"use_amp\", 0))\n",
        "    if DEVICE.type == \"cuda\" and use_amp:\n",
        "        # prefer torch.autocast if available\n",
        "        if hasattr(torch, \"autocast\"):\n",
        "            return torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
        "        else:\n",
        "            from torch.cuda.amp import autocast\n",
        "            return autocast()\n",
        "    return nullcontext()\n",
        "\n",
        "# derived parameters\n",
        "reprW, reprH = Nx * pixelSize, Ny * pixelSize\n",
        "crs_delta = 0.0001 * calc_perf_every_it\n",
        "maskOffset = OffsetMultiplier * np.sqrt(1e-3 / (Nx * Ny * n_of_modes))\n",
        "\n",
        "# wavelength-independent grids\n",
        "nx_m = pixelSize*np.linspace(-(Nx-1)/2, (Nx-1)/2, num=Nx)\n",
        "ny_m = pixelSize*np.linspace(-(Ny-1)/2, (Ny-1)/2, num=Ny)\n",
        "X,Y = np.meshgrid(nx_m,ny_m)\n",
        "X_torch = torch.from_numpy(X).to(DEVICE)\n",
        "Y_torch = torch.from_numpy(Y).to(DEVICE)\n",
        "\n",
        "nx = np.linspace(-(Nx-1)/2, (Nx-1)/2, num=Nx)\n",
        "ny = np.linspace(-(Ny-1)/2, (Ny-1)/2, num=Ny)\n",
        "kx, ky = np.meshgrid(2*np.pi*nx/(Nx*pixelSize),2*np.pi*ny/(Ny*pixelSize))\n",
        "\n",
        "\n",
        "lambda_list = np.array([1.53e-6, 1.55e-6, 1.57e-6, 1.59e-6, 1.61e-6, 1.625e-6], dtype=np.float64)\n",
        "lambda_c = 1.57e-6\n",
        "\n",
        "# 读取LP模式和高斯输出（多波长）\n",
        "lp_data = np.load('modes_lp_10.npz')\n",
        "lp_modes = lp_data['profiles']  # 形状: (L, 10, 512, 512)\n",
        "gauss_data = np.load('gauss_5x2_custom.npz')\n",
        "gauss_modes = gauss_data['profiles']  # 形状: (L, 10, 512, 512)\n",
        "\n",
        "L = min(lp_modes.shape[0], gauss_modes.shape[0], len(lambda_list))\n",
        "lambda_list = lambda_list[:L]\n",
        "\n",
        "Speckle_basis = lp_modes[:L, 0:n_of_modes, :, :].astype(np.complex64)\n",
        "Gaussian_basis = gauss_modes[:L, 0:n_of_modes, :, :].astype(np.complex64)\n",
        "Speckle_basis_torch = torch.from_numpy(Speckle_basis).to(DEVICE)\n",
        "Gaussian_basis_torch = torch.from_numpy(Gaussian_basis).to(DEVICE)\n",
        "\n",
        "# 生成多波长高斯mask\n",
        "Gaussian_Masks = np.zeros_like(Gaussian_basis, dtype=np.float32)\n",
        "for l in range(L):\n",
        "    for m in range(n_of_modes):\n",
        "        inten = np.abs(Gaussian_basis[l, m, :, :]) ** 2\n",
        "        thr = 0.05 * np.max(inten)\n",
        "        Gaussian_Masks[l, m, :, :] = inten > thr\n",
        "Gaussian_Masks_torch = torch.from_numpy(Gaussian_Masks).to(torch.float32).to(DEVICE)\n",
        "\n",
        "# 若需要pad\n",
        "if (Nx > 512) or (Ny > 512):\n",
        "    pad_x = int((Nx-512)/2)\n",
        "    pad_y = int((Ny-512)/2)\n",
        "    Speckle_basis_torch = nn.functional.pad(Speckle_basis_torch, (pad_x, Nx-512-pad_x, pad_y, Ny-512-pad_y), mode='constant', value=0.+0.j)\n",
        "    Gaussian_basis_torch = nn.functional.pad(Gaussian_basis_torch, (pad_x, Nx-512-pad_x, pad_y, Ny-512-pad_y), mode='constant', value=0.+0.j)\n",
        "    Gaussian_Masks_torch = nn.functional.pad(Gaussian_Masks_torch, (pad_x, Nx-512-pad_x, pad_y, Ny-512-pad_y), mode='constant', value=0.0)\n",
        "\n",
        "# 多波长下的 phi_bk 与 phi_cr（二值并集/补集定义，避免负值；在掩膜互斥时等价且更稳健）\n",
        "sum_masks = torch.sum(Gaussian_Masks_torch, dim=1)  # (L, Ny, Nx)\n",
        "phi_bk = (sum_masks == 0).to(torch.float32)  # 背景：未被任何通道覆盖\n",
        "phi_cr = ((sum_masks.unsqueeze(1) - Gaussian_Masks_torch) > 0).to(torch.float32)  # 交叉区域：其他通道的并集\n",
        "\n",
        "phi = Gaussian_basis_torch\n",
        "\n",
        "# # visualize one of the input modes, a set of Gaussians on the outputs and a binary mask outlining the backgroud region\n",
        "# # brightness = amplitude, colour = phase\n",
        "# plt.title(\"One of the input modes - $\\chi_{0}$\")\n",
        "# complim(Speckle_basis_torch[0, :, :])\n",
        "\n",
        "# plt.title(\"Sum of the output modes - $\\sum\\phi_{i}$\")\n",
        "# complim(torch.sum(phi, dim = 0))\n",
        "\n",
        "# plt.title(\"$\\phi^{bk}$\")\n",
        "# complim(phi_bk)\n",
        "\n",
        "# plt.title(\"$\\phi_{0}^{cr}$\")\n",
        "# complim(phi_cr[0,:,:])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Masks = torch.zeros((Planes, Ny, Nx), dtype=torch.float32, device=DEVICE)  # use zero phases as starting guesses for the phase masks\n",
        "Masks_complex = torch.exp(1j * Masks)  # complex representation of the phase masks with amplitude = 1 everywhere\n",
        "\n",
        "# create placeholder arrays to store every input and every output field in each plane\n",
        "L = Gaussian_Masks_torch.shape[0]\n",
        "Modes_in = torch.zeros((L, Planes, n_of_modes, Ny, Nx), dtype=torch.complex64, device=DEVICE)\n",
        "Modes_out = torch.zeros((L, Planes, n_of_modes, Ny, Nx), dtype=torch.complex64, device=DEVICE)\n",
        "\n",
        "overlap = torch.zeros((n_of_modes), dtype=torch.complex64, device=DEVICE)\n",
        "eff_distribution = torch.ones((n_of_modes), dtype=torch.float32, device=DEVICE)\n",
        "dFdpsi = torch.zeros((L, Planes, n_of_modes, Ny, Nx), dtype=torch.complex64, device=DEVICE)\n",
        "crs_array_convergence = torch.zeros((iterations//calc_perf_every_it), dtype = torch.double, device=DEVICE)\n",
        "conv_count = 0\n",
        "\n",
        "# 每个波长的 kz，初始化 Modes_in/Out\n",
        "kz_torch_list = []\n",
        "for l in range(L):\n",
        "    k_l = (2*np.pi)/lambda_list[l]\n",
        "    kz_l = np.lib.scimath.sqrt(k_l**2 - (kx**2 + ky**2)).astype(np.complex64)\n",
        "    kz_torch_list.append(torch.from_numpy(kz_l).to(DEVICE))\n",
        "    Modes_in[l, 0, :, :, :] = propagate_HK(Speckle_basis_torch[l], kz_torch_list[l], d_in)\n",
        "    # 目标场定义在输出面（距最后一面 d_out 处），用于反向传播到最后一面\n",
        "    Modes_out[l, Planes-1, :, :, :] = propagate_HK(phi[l], kz_torch_list[l], -d_out)\n",
        "\n",
        "# iterate \n",
        "for i in range(1, iterations+1):\n",
        "\n",
        "    # change the step size depending on the current iteration number\n",
        "    if i < first_n_iterations:\n",
        "        delta_theta = delta_theta_0\n",
        "    else:\n",
        "        delta_theta = delta_theta_1\n",
        "\n",
        "    # update all the phase masks on this iteration in an ascending order\n",
        "    for mask_ind in range(Planes):\n",
        "\n",
        "        # 多波长：按 λ 比例缩放相位并分别前后传播\n",
        "        for l in range(L):\n",
        "            with autocast_if_cuda():\n",
        "                scale_l = lambda_c / lambda_list[l]\n",
        "                # 预计算所有相位面的复指数，避免重复 exp\n",
        "                mask_cmplx_all = [torch.exp(1j*(Masks[pl, :, :]*scale_l)) for pl in range(Planes)]\n",
        "                modes = torch.zeros((n_of_modes, Ny, Nx), dtype = torch.complex64, device=DEVICE)\n",
        "                for pl in range(Planes-1):\n",
        "                    modes = Modes_in[l, pl, :, :, :] * mask_cmplx_all[pl]\n",
        "                    modes = propagate_HK(modes, kz_torch_list[l], d)\n",
        "                    Modes_in[l, pl+1, :, :, :] = modes\n",
        "                modes_forw_last_plane = Modes_in[l, Planes-1, :, :, :] * mask_cmplx_all[Planes-1]\n",
        "                # 从最后一面向前传播到真实输出面 z_out\n",
        "                eout_l = propagate_HK(modes_forw_last_plane, kz_torch_list[l], d_out)\n",
        "\n",
        "            for j in range(n_of_modes):\n",
        "                overlap = torch.sum(torch.squeeze(eout_l[j,:,:]) * torch.conj(torch.squeeze(phi[l,j,:,:])))\n",
        "                a = (phi[l, j, :, :]) * overlap\n",
        "                psi_cr_l = (torch.squeeze(eout_l[j,:,:])) * torch.squeeze(phi_cr[l,j,:,:])\n",
        "                psi_bk_l = (torch.squeeze(eout_l[j,:,:])) * phi_bk[l]\n",
        "                dFdpsi[l, Planes-1, j, :, :] = - alpha*a + (beta*psi_cr_l - gamma*psi_bk_l)*0.5\n",
        "\n",
        "            # 将输出面上的梯度场反向传播回最后一面\n",
        "            dFdpsi[l, Planes-1, :, :, :] = propagate_HK(dFdpsi[l, Planes-1, :, :, :], kz_torch_list[l], -d_out)\n",
        "\n",
        "            for pl in range(Planes-1, mask_ind, -1):\n",
        "                dFdpsi_prop = dFdpsi[l, pl, :, :, :] * torch.conj(mask_cmplx_all[pl])\n",
        "                dFdpsi_prop = propagate_HK(dFdpsi_prop, kz_torch_list[l], -d)\n",
        "                dFdpsi[l, pl-1, :, :, :] = dFdpsi_prop\n",
        "\n",
        "                phi_prop = Modes_out[l, pl, :, :, :] * torch.conj(mask_cmplx_all[pl])\n",
        "                phi_prop = propagate_HK(phi_prop, kz_torch_list[l], -d)\n",
        "                Modes_out[l, pl-1, :, :, :] = phi_prop\n",
        "\n",
        "        # if equalize_efficiency is on, make a sum in (1) a weighted sum, where the weights are 1/(relative_efficiency_i) for each particular mode            \n",
        "        if equalize_efficiency == 1:\n",
        "            total_term = torch.zeros((Ny, Nx), dtype=torch.complex64, device=DEVICE)\n",
        "            inv_eff = (1.0 / eff_distribution).view(n_of_modes, 1, 1)  # (M,1,1)\n",
        "            for l in range(L):\n",
        "                scale_l = lambda_c / lambda_list[l]\n",
        "                mask_cmplx_l = torch.exp(1j*(Masks[mask_ind, :, :]*scale_l))\n",
        "                Mi = Modes_in[l, mask_ind]  # (M, Ny, Nx)\n",
        "                Gi = dFdpsi[l, mask_ind]    # (M, Ny, Nx)\n",
        "                weighted_overlaps = torch.sum(inv_eff * Mi * torch.conj(Gi), dim=0)  # (Ny, Nx)\n",
        "                total_term = total_term + mask_cmplx_l * weighted_overlaps\n",
        "            delta_P = delta_theta*torch.sign(torch.imag(total_term))\n",
        "        else:\n",
        "            total_term = torch.zeros((Ny, Nx), dtype=torch.complex64, device=DEVICE)\n",
        "            for l in range(L):\n",
        "                scale_l = lambda_c / lambda_list[l]\n",
        "                mask_cmplx_l = torch.exp(1j*(Masks[mask_ind, :, :]*scale_l))\n",
        "                Mi = Modes_in[l, mask_ind]   # (M, Ny, Nx)\n",
        "                Gi = dFdpsi[l, mask_ind]     # (M, Ny, Nx)\n",
        "                overlaps = torch.sum(Mi * torch.conj(Gi), dim=0)\n",
        "                total_term = total_term + mask_cmplx_l * overlaps\n",
        "            delta_P = delta_theta*torch.sign(torch.imag(total_term))\n",
        "        \n",
        "        #  if smoothing_switch is on, mask the regions of the phase masks where there is almost no incedent light, based on the overlap of input and output modes at this plane\n",
        "        if smoothing_switch == 1:\n",
        "                ov_sum = torch.zeros((Ny, Nx), dtype=torch.float32, device=DEVICE)\n",
        "                for l in range(L):\n",
        "                    ov_sum = ov_sum + torch.abs(torch.sum(torch.squeeze(Modes_in[l, mask_ind, :, :, :] * torch.conj(Modes_out[l, mask_ind, :, :, :])), dim=0))\n",
        "                ovrlp_in_out = ov_sum / L\n",
        "                mask_cmplx = ovrlp_in_out * torch.exp(1j * (Masks[mask_ind, :, :] + delta_P))\n",
        "                # add a tiny real offset in a dtype/device-safe way (optional smoothing bias)\n",
        "                if maskOffset != 0:\n",
        "                    mask_cmplx = mask_cmplx + torch.tensor(maskOffset, dtype=torch.float32, device=DEVICE)\n",
        "                Masks[mask_ind, :, :] = torch.angle(mask_cmplx)\n",
        "        #  if smoothing_switch is off, just add phase delta_P to a current guess of the certain phase mask\n",
        "        else:\n",
        "            Masks[mask_ind, :, :] = Masks[mask_ind, :, :] + delta_P\n",
        "\n",
        "        # store the resulting current guess of the phase mask as a complex array, with amplitude = 1 everywhere\n",
        "    Masks_complex[mask_ind, :, :] = torch.exp(1j * torch.squeeze(Masks[mask_ind, :, :]))\n",
        "\n",
        "\n",
        "    # calculate and print out sorter's performance after every iteration (or every K iterations to save time)\n",
        "    if i % calc_perf_every_it == 0:\n",
        "        fids = []\n",
        "        crss = []\n",
        "        effs = []\n",
        "        eff_lists = []\n",
        "        for l in range(L):\n",
        "            with autocast_if_cuda():\n",
        "                scale_l = lambda_c / lambda_list[l]\n",
        "                # 复用缓存到最后一面的前向场，避免从 p0 重算\n",
        "                modes_last = Modes_in[l, Planes-1, :, :, :]\n",
        "                modes_last = modes_last * torch.exp(1j*(Masks[Planes-1, :, :]*scale_l))\n",
        "                eout = propagate_HK(modes_last, kz_torch_list[l], d_out)\n",
        "                eout_int_only = (torch.abs(eout))**2\n",
        "                fid, _ = performance_loc_fidelity(eout, Gaussian_Masks_torch[l], phi[l]) \n",
        "                crs, _, _ = performance_crosstalk(eout_int_only, Gaussian_Masks_torch[l]) \n",
        "                eff, eff_list = performance_efficiency(eout_int_only, Gaussian_Masks_torch[l])\n",
        "            fids.append(fid); crss.append(crs); effs.append(eff)\n",
        "            eff_lists.append(eff_list)\n",
        "\n",
        "        fid = torch.stack(fids).mean(); crs = torch.stack(crss).mean(); eff = torch.stack(effs).mean()\n",
        "        print('iteration', i, ': loc. fidelity =', round(fid.detach().cpu().numpy().item(),2), ', crosstalk =', round(crs.detach().cpu().numpy().item(),2), ', efficiency =', round(eff.detach().cpu().numpy().item(),2))\n",
        "        crs_array_convergence[conv_count] = crs # store calculated cross-talk to an array to then plot it against the number of iterations\n",
        "        \n",
        "        # stop iterating if the algorithm is no longer improving cross-talk by more than a certain value after a certain iteration\n",
        "        if (conv_count > 0) and (i > (iterations/3)) and ((crs_array_convergence[conv_count-1] - crs_array_convergence[conv_count]) < crs_delta):\n",
        "            break\n",
        "        conv_count = conv_count + 1\n",
        "\n",
        "        # store a list of a relative efficiency of every output on the current iteration to try to equalize them on the next run\n",
        "        if equalize_efficiency == 1:\n",
        "            # 跨波长聚合（均值），使均衡对所有 λ 公平；可按需改为中位数\n",
        "            eff_stack = torch.stack(eff_lists, dim=0)  # (L, M)\n",
        "            eff_mean = torch.mean(eff_stack, dim=0)\n",
        "            eff_distribution = torch.clamp(eff_mean / torch.max(eff_mean), min=1e-6)\n",
        "            # plot efficiency distribution if plot_eff_distribution is on\n",
        "            if plot_eff_distribution == 1:                    \n",
        "                plt.plot(eff_distribution)\n",
        "                plt.title('efficiency distribution')\n",
        "                plt.ylim((0,1))\n",
        "                plt.show()\n",
        "        \n",
        "fids = []; crss = []; effs = []\n",
        "for l in range(L):\n",
        "    with autocast_if_cuda():\n",
        "        scale_l = lambda_c / lambda_list[l]\n",
        "        # 复用缓存到最后一面的前向场，避免从 p0 重算\n",
        "        modes_last = Modes_in[l, Planes-1, :, :, :]\n",
        "        modes_last = modes_last * torch.exp(1j*(Masks[Planes-1, :, :]*scale_l))\n",
        "        eout = propagate_HK(modes_last, kz_torch_list[l], d_out)\n",
        "    eout_int_only = (torch.abs(eout))**2\n",
        "    fid, _ = performance_loc_fidelity(eout, Gaussian_Masks_torch[l], phi[l])\n",
        "    crs, _, _ = performance_crosstalk(eout_int_only, Gaussian_Masks_torch[l])\n",
        "    eff, _ = performance_efficiency(eout_int_only, Gaussian_Masks_torch[l])\n",
        "    fids.append(fid); crss.append(crs); effs.append(eff)\n",
        "fid = torch.stack(fids).mean(); crs = torch.stack(crss).mean(); eff = torch.stack(effs).mean()\n",
        "print('Final performance (avg over λ): loc. fidelity =', round(fid.detach().cpu().numpy().item(),3), ', crosstalk =', round(crs.detach().cpu().numpy().item(),3), ', efficiency =', round(eff.detach().cpu().numpy().item(),3))\n",
        "\n",
        "if CFG.get(\"plot_results\", 0) == 1:\n",
        "    # 展示相位面\n",
        "    for i in range(Planes):\n",
        "        plt.title(\"Phase mask %s\" %(i+1))\n",
        "        _ = plot_in_GS(Masks[i,:,:])\n",
        "\n",
        "# 逐波长性能打印\n",
        "for idx, (f_i, c_i, e_i) in enumerate(zip(fids, crss, effs)):\n",
        "    print(f\"λ={lambda_list[idx]*1e6:.3f} µm -> fidelity={float(f_i.detach().cpu().numpy()):.3f}, crosstalk={float(c_i.detach().cpu().numpy()):.3f}, efficiency={float(e_i.detach().cpu().numpy()):.3f}\")\n",
        "\n",
        "if CFG.get(\"do_padded_eval\", 0) == 1:\n",
        "    newNx = Nx + 400\n",
        "    newNy = Ny + 400\n",
        "    l_c = 2 if L >= 3 else 0\n",
        "    # 注意维度顺序：(Ny, Nx)\n",
        "    Modes_in_wide = torch.zeros((Planes, n_of_modes, newNy, newNx), dtype=torch.complex64, device=DEVICE)\n",
        "    Modes_in_wide[0,:,200:200+Ny,200:200+Nx] = Modes_in[l_c,0,:,:,:]\n",
        "    Masks_wide = torch.zeros((Planes, newNy, newNx), dtype=torch.float32, device=DEVICE)\n",
        "    Masks_complex_wide = torch.exp(1j * Masks_wide)\n",
        "    Masks_complex_wide[:,200:200+Ny,200:200+Nx] = Masks_complex\n",
        "    nx_wide = np.linspace(-(newNx-1)/2, (newNx-1)/2, num=newNx)\n",
        "    ny_wide = np.linspace(-(newNy-1)/2, (newNy-1)/2, num=newNy)\n",
        "    kx_wide, ky_wide = np.meshgrid(2*np.pi*nx_wide/(newNx*pixelSize),2*np.pi*ny_wide/(newNy*pixelSize))\n",
        "    kz_wide = np.lib.scimath.sqrt((2*np.pi/lambda_c)**2 - (kx_wide**2 + ky_wide**2)).astype(np.complex64)\n",
        "    kz_torch_wide = torch.from_numpy(kz_wide).to(DEVICE)\n",
        "    for pl in range(Planes-1):\n",
        "        modes = Modes_in_wide[pl, :, :, :]*Masks_complex_wide[pl, :, :]\n",
        "        modes = propagate_HK(modes, kz_torch_wide, d)\n",
        "        Modes_in_wide[pl+1, :, :, :] = modes\n",
        "    modes = modes*Masks_complex_wide[Planes-1,:,:]\n",
        "    modes_cropped = modes[:,200:200+Ny,200:200+Nx]\n",
        "    # 在宽域上从最后一面传播到输出面，再裁剪评估\n",
        "    with autocast_if_cuda():\n",
        "        eout_wide = propagate_HK(modes, kz_torch_wide, d_out)\n",
        "    eout_cropped = eout_wide[:,200:200+Ny,200:200+Nx]\n",
        "    eout_cropped_int_only = (torch.abs(eout_cropped))**2\n",
        "    fid_wide, _ = performance_loc_fidelity(eout_cropped, Gaussian_Masks_torch[l_c], phi[l_c])\n",
        "    crs_wide, _, _ = performance_crosstalk(eout_cropped_int_only, Gaussian_Masks_torch[l_c])\n",
        "    eff_wide, _ = performance_efficiency(eout_cropped_int_only, Gaussian_Masks_torch[l_c])\n",
        "    print('performance padded (λc): loc. fidelity =', round(fid_wide.detach().cpu().numpy().item(),3), ', crosstalk =', round(crs_wide.detach().cpu().numpy().item(),3), ', efficiency =', round(eff_wide.detach().cpu().numpy().item(),3))\n",
        "\n",
        "    plt.plot(crs_array_convergence)\n",
        "    plt.ylabel('avg. crosstalk (avg over λ)')\n",
        "    plt.xlabel('iterations/(calc_perf_every_it)')\n",
        "    plt.axis([0, iterations//calc_perf_every_it, 0, 20])\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5960\\88497243.py:152: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n",
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5960\\88497243.py:239: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wavelengths (μm): ['1.530', '1.550', '1.570', '1.590', '1.610', '1.625']\n",
            "IL (dB):          ['-3.034', '-2.414', '-2.141', '-2.130', '-2.395', '-2.878']\n",
            "MDL (dB):         ['2.030', '1.393', '1.159', '1.260', '1.579', '2.102']\n",
            "XTs_avg (dB):     ['-18.640', '-21.652', '-23.617', '-23.502', '-21.299', '-18.931']\n",
            "fidelity:         ['92.702', '93.863', '94.265', '94.245', '93.806', '93.017']\n",
            "crosstalk:        ['3.115', '1.805', '1.357', '1.306', '1.619', '2.390']\n",
            "efficiency:       ['45.908', '53.171', '56.807', '57.160', '54.060', '48.503']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5960\\88497243.py:289: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# Visualization: λ=1.57 μm 前/后向“相位前”分布与相位图\n",
        "# - 前向快照: z=0, p0..p6 的 pre-phase (传播到该面, 未乘该面相位), 以及 z_out\n",
        "# - 后向快照: z_out, p6..p0 的 pre-phase (从后向传播到该面, 未乘该面相位), 以及 z=0\n",
        "# ==========================================\n",
        "import os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # 选择 λ=1.57 μm 的索引\n",
        "    l_idx = int(np.argmin(np.abs(lambda_list - lambda_c)))\n",
        "    kz_l = kz_torch_list[l_idx]\n",
        "    scale_l = lambda_c / lambda_list[l_idx]\n",
        "\n",
        "    # 前向: 收集相位前快照（总强度=所有模式强度求和）\n",
        "    fwd_titles = []\n",
        "    fwd_maps = []\n",
        "    # z=0\n",
        "    modes = Speckle_basis_torch[l_idx].clone()\n",
        "    fwd_maps.append(torch.sum(torch.abs(modes) ** 2, dim=0))\n",
        "    fwd_titles.append('z=0')\n",
        "    # 传播到 p0 (pre-phase)\n",
        "    modes = propagate_HK(modes, kz_l, d_in)\n",
        "    fwd_maps.append(torch.sum(torch.abs(modes) ** 2, dim=0))\n",
        "    fwd_titles.append('p0 pre')\n",
        "    # 依次到 p1..p6 的 pre-phase\n",
        "    for pl in range(0, Planes-1):  # 到 p1..p6 的pre，需要先在上一面乘相位再传播\n",
        "        mask_cmplx = torch.exp(1j * (Masks[pl] * scale_l))\n",
        "        modes = modes * mask_cmplx\n",
        "        modes = propagate_HK(modes, kz_l, d)\n",
        "        fwd_maps.append(torch.sum(torch.abs(modes) ** 2, dim=0))\n",
        "        fwd_titles.append(f'p{pl+1} pre')\n",
        "    # 输出面 z_out (在 p6 乘相位后传播 d_out)\n",
        "    modes = modes * torch.exp(1j * (Masks[Planes-1] * scale_l))\n",
        "    modes_out = propagate_HK(modes, kz_l, d_out)\n",
        "    fwd_maps.append(torch.sum(torch.abs(modes_out) ** 2, dim=0))\n",
        "    fwd_titles.append('z_out')\n",
        "\n",
        "    # 后向: 从目标输出面场出发，收集各面的 pre-phase\n",
        "    bwd_titles = []\n",
        "    bwd_maps = []\n",
        "    # z_out（目标场）\n",
        "    modes_b = phi[l_idx].clone()\n",
        "    bwd_maps.append(torch.sum(torch.abs(modes_b) ** 2, dim=0))\n",
        "    bwd_titles.append('z_out')\n",
        "    # 到 p6 pre：先 -d_out 到 p6 的后相位(post)，再乘 conj(mask6) 得 pre\n",
        "    modes_b = propagate_HK(modes_b, kz_l, -d_out)\n",
        "    mask6 = torch.exp(1j * (Masks[Planes-1] * scale_l))\n",
        "    modes_b = modes_b * torch.conj(mask6)\n",
        "    bwd_maps.append(torch.sum(torch.abs(modes_b) ** 2, dim=0))\n",
        "    bwd_titles.append('p6 pre')\n",
        "    # 依次到 p5..p0 的 pre：每步先 -d 到达上一面的 post，再乘对应 conj(mask) 得 pre\n",
        "    for pl in range(Planes-2, -1, -1):  # from p5 down to p0\n",
        "        modes_b = propagate_HK(modes_b, kz_l, -d)\n",
        "        mask_cmplx = torch.exp(1j * (Masks[pl] * scale_l))\n",
        "        modes_b = modes_b * torch.conj(mask_cmplx)\n",
        "        bwd_maps.append(torch.sum(torch.abs(modes_b) ** 2, dim=0))\n",
        "        bwd_titles.append(f'p{pl} pre')\n",
        "    # 最后到 z=0：-d_in 传播\n",
        "    modes_b = propagate_HK(modes_b, kz_l, -d_in)\n",
        "    bwd_maps.append(torch.sum(torch.abs(modes_b) ** 2, dim=0))\n",
        "    bwd_titles.append('z=0')\n",
        "\n",
        "    # 画图：前向（自适应子图数量）\n",
        "    import matplotlib.pyplot as plt\n",
        "    nplots_fwd = len(fwd_maps)\n",
        "    ncols = 4\n",
        "    nrows = math.ceil(nplots_fwd / ncols)\n",
        "    fig1, axes1 = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
        "    axes1_flat = np.array(axes1).ravel() if isinstance(axes1, np.ndarray) else np.array([axes1])\n",
        "    for idx in range(nplots_fwd):\n",
        "        ax = axes1_flat[idx]\n",
        "        im = ax.imshow(fwd_maps[idx].detach().cpu().numpy(), cmap='inferno', origin='lower')\n",
        "        ax.set_title(fwd_titles[idx])\n",
        "        ax.axis('off')\n",
        "    for k in range(nplots_fwd, nrows*ncols):\n",
        "        axes1_flat[k].axis('off')\n",
        "    fig1.suptitle('Forward pre-phase intensity (λ=1.57 μm)')\n",
        "    fig1.tight_layout()\n",
        "    fig1.savefig('results/forward_prephase_1p57.png', dpi=150)\n",
        "\n",
        "    # 画图：后向（自适应子图数量）\n",
        "    nplots_bwd = len(bwd_maps)\n",
        "    ncols = 4\n",
        "    nrows = math.ceil(nplots_bwd / ncols)\n",
        "    fig2, axes2 = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
        "    axes2_flat = np.array(axes2).ravel() if isinstance(axes2, np.ndarray) else np.array([axes2])\n",
        "    for idx in range(nplots_bwd):\n",
        "        ax = axes2_flat[idx]\n",
        "        im = ax.imshow(bwd_maps[idx].detach().cpu().numpy(), cmap='inferno', origin='lower')\n",
        "        ax.set_title(bwd_titles[idx])\n",
        "        ax.axis('off')\n",
        "    for k in range(nplots_bwd, nrows*ncols):\n",
        "        axes2_flat[k].axis('off')\n",
        "    fig2.suptitle('Backward pre-phase intensity (λ=1.57 μm)')\n",
        "    fig2.tight_layout()\n",
        "    fig2.savefig('results/backward_prephase_1p57.png', dpi=150)\n",
        "\n",
        "    # 三行 overview：第一行前向，第二行后向按前向顺序反着放（无标题），第三行掩膜居中（少两个，隐藏空位坐标轴）\n",
        "    ncols_ovr = len(fwd_maps)\n",
        "    fig_ovr, axes_ovr = plt.subplots(3, ncols_ovr, figsize=(3*ncols_ovr, 9))\n",
        "    # row 0: forward\n",
        "    for c in range(ncols_ovr):\n",
        "        ax = axes_ovr[0, c]\n",
        "        ax.imshow(fwd_maps[c].detach().cpu().numpy(), cmap='inferno', origin='lower')\n",
        "        ax.set_title(fwd_titles[c])\n",
        "        ax.axis('off')\n",
        "    # row 1: backward, reversed to align positions with forward (no titles)\n",
        "    bwd_aligned = list(reversed(bwd_maps))\n",
        "    for c in range(min(ncols_ovr, len(bwd_aligned))):\n",
        "        ax = axes_ovr[1, c]\n",
        "        ax.imshow(bwd_aligned[c].detach().cpu().numpy(), cmap='inferno', origin='lower')\n",
        "        ax.axis('off')\n",
        "    # row 2: masks centered (Planes = ncols_ovr - 2)\n",
        "    start = 1 if ncols_ovr >= 2 else 0\n",
        "    for p in range(Planes):\n",
        "        c = start + p\n",
        "        if c < ncols_ovr:\n",
        "            ax = axes_ovr[2, c]\n",
        "            ax.imshow(Masks[p].detach().cpu().numpy(), cmap='twilight', origin='lower')\n",
        "            ax.axis('off')  # 第三行标题略去\n",
        "    # 关闭空白子图（第一行、第二行多余列，以及第三行两侧空位）\n",
        "    for c in range(ncols_ovr):\n",
        "        if c >= len(fwd_maps):\n",
        "            axes_ovr[0, c].axis('off')\n",
        "        if c >= len(bwd_aligned):\n",
        "            axes_ovr[1, c].axis('off')\n",
        "        if c < start or c >= start + Planes:\n",
        "            axes_ovr[2, c].axis('off')\n",
        "    fig_ovr.suptitle('Overview: forward / backward(aligned) / masks')\n",
        "    fig_ovr.tight_layout()\n",
        "    fig_ovr.savefig('results/overview_prephase.png', dpi=150)\n",
        "\n",
        "    # 相位图：相位面数量自适应（按每行 4 列排布）\n",
        "    ncols = 4\n",
        "    nrows = math.ceil(Planes / ncols)\n",
        "    fig3, axes3 = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
        "    # 将 axes 拉平成 1D，便于按索引逐个填充\n",
        "    axes3_flat = np.array(axes3).ravel() if isinstance(axes3, np.ndarray) else np.array([axes3])\n",
        "    for p in range(Planes):\n",
        "        ax = axes3_flat[p]\n",
        "        ax.imshow(Masks[p].detach().cpu().numpy(), cmap='twilight', origin='lower')\n",
        "        ax.set_title(f'Mask p{p}')\n",
        "        ax.axis('off')\n",
        "    # 关掉多余子图\n",
        "    for k in range(Planes, nrows*ncols):\n",
        "        axes3_flat[k].axis('off')\n",
        "    fig3.suptitle('Phase masks (radians)')\n",
        "    fig3.tight_layout()\n",
        "    fig3.savefig('results/masks_phase_maps.png', dpi=150)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Subplot: 六个波长的耦合矩阵 + 指标 (IL, MDL, XTs_avg_dB, fidelity/crosstalk/efficiency)\n",
        "# - 耦合矩阵基于输出面复场与目标复场的归一化内积 C_{m,j}=\n",
        "#   <E_out_m, Phi_j>/sqrt(<E_out_m,E_out_m><Phi_j,Phi_j>)\n",
        "# - IL=10*log10(mean(s^2)), MDL=10*log10(max(s^2)/min(s^2)), XTs_avg_dB=10*log10(mean(((sum|C|^2 - diag|C|^2)/diag|C|^2)))\n",
        "# ==========================================\n",
        "with torch.no_grad():\n",
        "    Nl = len(lambda_list)\n",
        "    modeCount = n_of_modes\n",
        "    ILs = np.zeros(Nl)\n",
        "    MDLs = np.zeros(Nl)\n",
        "    XTs_avg_dB = np.zeros(Nl)\n",
        "    fids_l = np.zeros(Nl)\n",
        "    crss_l = np.zeros(Nl)\n",
        "    effs_l = np.zeros(Nl)\n",
        "\n",
        "    # 预创建子图（自适应 Nl 个波长）\n",
        "    ncols = 3\n",
        "    nrows = math.ceil(Nl / ncols)\n",
        "    fig_cm, axes_cm = plt.subplots(nrows, ncols, figsize=(4*ncols, 3.5*nrows))\n",
        "    axes_cm_flat = np.array(axes_cm).ravel() if isinstance(axes_cm, np.ndarray) else np.array([axes_cm])\n",
        "\n",
        "    for idx in range(nrows * ncols):\n",
        "        ax = axes_cm_flat[idx]\n",
        "        if idx >= Nl:\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "        l = idx\n",
        "        kz_l = kz_torch_list[l]\n",
        "        scale_l = lambda_c / lambda_list[l]\n",
        "\n",
        "        # 前向到输出面\n",
        "        modes = propagate_HK(Speckle_basis_torch[l], kz_l, d_in)\n",
        "        for pl in range(Planes-1):\n",
        "            modes = modes * torch.exp(1j * (Masks[pl] * scale_l))\n",
        "            modes = propagate_HK(modes, kz_l, d)\n",
        "        modes = modes * torch.exp(1j * (Masks[Planes-1] * scale_l))\n",
        "        eout = propagate_HK(modes, kz_l, d_out)  # (M, Ny, Nx)\n",
        "\n",
        "        # 基于复内积构建耦合矩阵 C (M×M)\n",
        "        E = eout.reshape(modeCount, -1)\n",
        "        P = phi[l].reshape(modeCount, -1)\n",
        "        num = E @ torch.conj(P).T  # (M,M)\n",
        "        normE = torch.sum(torch.abs(E)**2, dim=1)  # (M,)\n",
        "        normP = torch.sum(torch.abs(P)**2, dim=1)  # (M,)\n",
        "        denom = torch.sqrt(normE[:, None] * normP[None, :]) + 1e-12\n",
        "        C = num / denom\n",
        "\n",
        "        # IL / MDL from SVD of C\n",
        "        C_np = C.detach().cpu().numpy()\n",
        "        s = np.linalg.svd(C_np, compute_uv=False)  # singular values\n",
        "        s2 = s**2\n",
        "        ILs[l] = 10 * np.log10(np.mean(s2))\n",
        "        MDLs[l] = 10 * np.log10(np.max(s2) / (np.min(s2) + 1e-15))\n",
        "\n",
        "        # XTs (per mode) and XTs_avg_dB\n",
        "        C2 = np.abs(C_np)**2\n",
        "        totalPower = np.sum(C2, axis=1)\n",
        "        signalPower = np.clip(np.diag(C2), 1e-15, None)\n",
        "        XTs_modes = (totalPower - signalPower) / signalPower\n",
        "        XTs_avg_dB[l] = 10 * np.log10(np.mean(XTs_modes))\n",
        "\n",
        "        # 同时计算 fidelity/crosstalk/efficiency（基于 mask 的原函数）\n",
        "        eout_int = (torch.abs(eout))**2\n",
        "        fid_l, _ = performance_loc_fidelity(eout, Gaussian_Masks_torch[l], phi[l])\n",
        "        crs_l, _, _ = performance_crosstalk(eout_int, Gaussian_Masks_torch[l])\n",
        "        eff_l, _ = performance_efficiency(eout_int, Gaussian_Masks_torch[l])\n",
        "        fids_l[l] = float(fid_l.detach().cpu().numpy())\n",
        "        crss_l[l] = float(crs_l.detach().cpu().numpy())\n",
        "        effs_l[l] = float(eff_l.detach().cpu().numpy())\n",
        "\n",
        "        # 绘制该波长的耦合矩阵（功率 |C|^2）\n",
        "        C2_plot = np.flip(C2, axis=1)  # 列翻转显示\n",
        "        ax.imshow(C2_plot, cmap='magma', origin='lower', aspect='equal')\n",
        "        ax.set_title(f'λ={lambda_list[l]*1e6:.3f} μm')\n",
        "        ax.axis('off')\n",
        "\n",
        "    # 关闭多余子图\n",
        "    for k in range(Nl, nrows*ncols):\n",
        "        axes_cm_flat[k].axis('off')\n",
        "\n",
        "    fig_cm.suptitle('Coupling matrices |C|^2 across wavelengths')\n",
        "    fig_cm.tight_layout()\n",
        "    fig_cm.savefig('results/coupling_matrices_6wls.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # 打印表格型结果（简洁版）\n",
        "    print('Wavelengths (μm):', [f'{wl*1e6:.3f}' for wl in lambda_list])\n",
        "    print('IL (dB):         ', [f'{v:.3f}' for v in ILs])\n",
        "    print('MDL (dB):        ', [f'{v:.3f}' for v in MDLs])\n",
        "    print('XTs_avg (dB):    ', [f'{v:.3f}' for v in XTs_avg_dB])\n",
        "    print('fidelity:        ', [f'{v:.3f}' for v in fids_l])\n",
        "    print('crosstalk:       ', [f'{v:.3f}' for v in crss_l])\n",
        "    print('efficiency:      ', [f'{v:.3f}' for v in effs_l])\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 追加可视化：λ=1.57 μm 时，10 个模式反向传播到 z=0 的强度图\n",
        "# 结果保存：results/backward_z0_modes_1p57.png\n",
        "# ==========================================\n",
        "with torch.no_grad():\n",
        "    import os\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    # 选择 λ=1.57 μm 对应索引与缩放\n",
        "    l_idx = int(np.argmin(np.abs(lambda_list - lambda_c)))\n",
        "    kz_l = kz_torch_list[l_idx]\n",
        "    scale_l = lambda_c / lambda_list[l_idx]\n",
        "\n",
        "    # 从目标面出发，逐面反向传播到 z=0（逐模式并行）\n",
        "    modes_b = phi[l_idx].clone()  # (M, Ny, Nx)\n",
        "    modes_b = propagate_HK(modes_b, kz_l, -d_out)\n",
        "    modes_b = modes_b * torch.conj(torch.exp(1j * (Masks[Planes-1] * scale_l)))\n",
        "    for pl in range(Planes-2, -1, -1):\n",
        "        modes_b = propagate_HK(modes_b, kz_l, -d)\n",
        "        modes_b = modes_b * torch.conj(torch.exp(1j * (Masks[pl] * scale_l)))\n",
        "    modes_b = propagate_HK(modes_b, kz_l, -d_in)  # at z=0\n",
        "\n",
        "    # 自适应每模式可视化（显示所有可用模式，不新增参数）\n",
        "    M = min(modes_b.shape[0], n_of_modes)\n",
        "    ncols = min(5, M) if M > 0 else 1\n",
        "    nrows = math.ceil(M / ncols) if M > 0 else 1\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
        "    axes_flat = np.array(axes).ravel() if isinstance(axes, np.ndarray) else np.array([axes])\n",
        "    for j in range(M):\n",
        "        inten = torch.abs(modes_b[j]) ** 2\n",
        "        axes_flat[j].imshow(inten.detach().cpu().numpy(), cmap='inferno', origin='lower')\n",
        "        axes_flat[j].set_title(f'mode {j} @ z=0')\n",
        "        axes_flat[j].axis('off')\n",
        "    for k in range(M, nrows*ncols):\n",
        "        axes_flat[k].axis('off')\n",
        "    fig.suptitle('Backward to z=0 per-mode intensity (λ=1.57 μm)')\n",
        "    fig.tight_layout()\n",
        "    fig.savefig('results/backward_z0_modes_1p57.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPqILUXObwXAeVJkmXx8z7q",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
