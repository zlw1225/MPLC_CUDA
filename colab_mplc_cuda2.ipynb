{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d174442",
   "metadata": {},
   "source": [
    "# 在 Google Colab 运行本项目（自动环境 & 数据获取）\n",
    "\n",
    "本笔记本会在全新 Colab 环境中完成：\n",
    "- 安装所需依赖（numpy, matplotlib, PyTorch 在 Colab 已自带）。\n",
    "- 可选挂载 Google Drive，从指定目录拷贝数据文件：modes_lp_10.npz、gauss_5x2_custom.npz。\n",
    "- 也可配置 GitHub 原始链接前缀，自动下载上述数据文件。\n",
    "- 将当前仓库中的 utils.py 与 MPLC_CUDA2.py 写入工作目录。\n",
    "- 提供一个快速“烟雾测试运行”（默认迭代较少，验证环境就绪），以及可选的“完整运行”。\n",
    "\n",
    "请先在下方“环境与依赖安装”与“数据来源配置”两节确认设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554d3ae",
   "metadata": {},
   "source": [
    "# 在 Google Colab 运行 MPLC_CUDA2（多波长）\n",
    "\n",
    "本 Notebook 会：\n",
    "- 安装所需依赖（最少化变更，优先使用 Colab 预装 PyTorch/CUDA）\n",
    "- 挂载 Google Drive 或从 GitHub 下载数据文件（modes_lp_10.npz、gauss_5x2_custom.npz）\n",
    "- 将项目核心脚本与工具写入当前运行环境（utils.py、MPLC_CUDA2.py）\n",
    "- 一键运行并展示结果图片（保存在 results/）\n",
    "\n",
    "如果你没有公共 GitHub 链接，请把数据文件放到 Drive 指定目录，或使用“手动上传”单元格上传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境与依赖安装（幂等）\n",
    "import sys, subprocess, os, json\n",
    "\n",
    "# Colab 里一般预装 torch 和 CUDA，可以按需升级/锁版本。\n",
    "req = [\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "]\n",
    "\n",
    "# 仅当缺失时安装，避免重复网络耗时\n",
    "for pkg in req:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "# 打印关键包与 CUDA 情况\n",
    "import numpy as np, matplotlib\n",
    "try:\n",
    "    import torch\n",
    "    print('[Env] torch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('[Env] CUDA device count:', torch.cuda.device_count())\n",
    "except Exception as e:\n",
    "    print('[Warn] torch import failed:', e)\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print('[OK] Base deps ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境与依赖安装（Colab）\n",
    "# - Colab 通常自带 torch/cuda、numpy、matplotlib。\n",
    "# - 若需要固定版本，可在下方解开注释安装。\n",
    "# - 自动检测 CUDA 可用性。\n",
    "\n",
    "import os, sys, subprocess, textwrap, json, shutil\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "try:\n",
    "    import torch, numpy as np, matplotlib\n",
    "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "\n",
    "# 可选：固定第三方版本（按需）\n",
    "# !pip -q install numpy==1.26.4 matplotlib==3.7.1\n",
    "\n",
    "# ========== 数据来源配置 ==========\n",
    "# 选项一：从 Google Drive 复制（推荐你先把 npz 放到 Drive 的某个目录）\n",
    "USE_DRIVE = True  # 如果不用 Drive，设为 False\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/mplc_data\"  # 你的 npz 所在目录\n",
    "\n",
    "# 选项二：从 GitHub 原始链接下载（把 raw 前缀改成你的仓库文件原始链接前缀）\n",
    "USE_GITHUB = False\n",
    "GITHUB_RAW_PREFIX = \"https://raw.githubusercontent.com/<your_user>/<your_repo>/<branch_or_tag>/\"  # 修改为你自己的\n",
    "\n",
    "DATA_FILES = [\n",
    "    (\"modes_lp_10.npz\", \"modes_lp_10.npz\"),\n",
    "    (\"gauss_5x2_custom.npz\", \"gauss_5x2_custom.npz\"),\n",
    "]\n",
    "\n",
    "# ========== 可选：挂载 Google Drive ==========\n",
    "if USE_DRIVE:\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"Drive mounted\")\n",
    "    except Exception as e:\n",
    "        print(\"Drive not available (non-Colab or no permission):\", e)\n",
    "        USE_DRIVE = False\n",
    "\n",
    "os.makedirs('/content/work', exist_ok=True)\n",
    "%cd /content/work\n",
    "\n",
    "# ========== 获取数据文件 ==========\n",
    "missing = []\n",
    "for local_name, drive_name in DATA_FILES:\n",
    "    if os.path.exists(local_name):\n",
    "        continue\n",
    "    copied = False\n",
    "    if USE_DRIVE:\n",
    "        src = os.path.join(DRIVE_DIR, drive_name)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, local_name)\n",
    "            print(f\"Copied from Drive: {src} -> {local_name}\")\n",
    "            copied = True\n",
    "        else:\n",
    "            print(f\"Not found in Drive: {src}\")\n",
    "    if (not copied) and USE_GITHUB:\n",
    "        import urllib.request\n",
    "        url = GITHUB_RAW_PREFIX.rstrip('/') + '/' + drive_name\n",
    "        try:\n",
    "            print(\"Downloading:\", url)\n",
    "            urllib.request.urlretrieve(url, local_name)\n",
    "            copied = True\n",
    "        except Exception as e:\n",
    "            print(\"Download failed:\", e)\n",
    "    if not copied:\n",
    "        missing.append(local_name)\n",
    "\n",
    "if missing:\n",
    "    print(\"WARNING: 下列数据文件未找到。请：\")\n",
    "    print(\"1) 打开上方 USE_DRIVE=True 并把文件放到 DRIVE_DIR；或\")\n",
    "    print(\"2) 打开 USE_GITHUB=True 并配置 GITHUB_RAW_PREFIX；或\")\n",
    "    print(\"3) 手动上传到左侧 Files 面板当前目录(/content/work)。\")\n",
    "    print(\"Missing:\", missing)\n",
    "else:\n",
    "    print(\"All data files ready.\")\n",
    "\n",
    "# ========== 写入 utils.py 与 MPLC_CUDA2.py ==========\n",
    "# 我们将把本仓库中的两个关键文件内容嵌入到 Colab 工作目录，避免 import 失败。\n",
    "\n",
    "UTILS_CODE = r\"\"\"\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import singledispatch\n",
    "from math import pi\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "__all__ = [\n",
    "    \"fft2\",\n",
    "    \"ifft2\",\n",
    "    \"normalize\",\n",
    "    \"propagate_HK\",\n",
    "    \"fidelity\",\n",
    "    \"loc_fidelity\",\n",
    "    \"performance_loc_fidelity\",\n",
    "    \"performance_efficiency\",\n",
    "    \"performance_crosstalk\",\n",
    "    \"complim\",\n",
    "    \"complim_subplot2\",\n",
    "    \"plot_in_GS\",\n",
    "]\n",
    "\n",
    "@singledispatch\n",
    "def fft2(x: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot fourier transform `x` for type: {type(x)}\")\n",
    "\n",
    "@fft2.register\n",
    "def _(x: np.ndarray) -> np.ndarray:\n",
    "    return np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(x, axes=(-1, -2)), norm=\"ortho\"), axes=(-1, -2))\n",
    "\n",
    "@fft2.register\n",
    "def fft2_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.fft.fftshift(torch.fft.fft2(torch.fft.ifftshift(x, dim=(-1, -2)), norm=\"ortho\"), dim=(-1, -2))\n",
    "\n",
    "@singledispatch\n",
    "def ifft2(x: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot Inverse fourier transform `x` for {type(x)}\")\n",
    "\n",
    "@ifft2.register\n",
    "def _(x: np.ndarray) -> np.ndarray:\n",
    "    return np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(x, axes=(-1, -2)), norm=\"ortho\"), axes=(-1, -2))\n",
    "\n",
    "@ifft2.register\n",
    "def ifft2_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.fft.fftshift(torch.fft.ifft2(torch.fft.ifftshift(x, dim=(-1, -2)), norm=\"ortho\"), dim=(-1, -2))\n",
    "\n",
    "@singledispatch\n",
    "def normalize(x: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot normalize for {type(x)}\")\n",
    "\n",
    "@normalize.register\n",
    "def _(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x / torch.linalg.norm(x)\n",
    "\n",
    "@normalize.register\n",
    "def _(x: np.ndarray) -> np.ndarray:\n",
    "    return x / np.linalg.norm(x)\n",
    "\n",
    "@singledispatch\n",
    "def propagate_HK(FieldIn: Union[np.ndarray, torch.Tensor], kz: Union[np.ndarray, torch.Tensor], distance: float = 0.0) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot process `FieldIn` type: {type(FieldIn)}\")\n",
    "\n",
    "@propagate_HK.register\n",
    "def _(FieldIn: np.ndarray, kz: np.ndarray, distance: float = 0.0) -> np.ndarray:\n",
    "    FieldIn_FT = fft2(FieldIn)\n",
    "    FieldOut_FT = FieldIn_FT*np.exp(1j*kz*distance)*(np.imag(kz)==0)\n",
    "    FieldOut = ifft2(FieldOut_FT)\n",
    "    return FieldOut\n",
    "\n",
    "@propagate_HK.register\n",
    "def _(FieldIn: torch.Tensor, kz: torch.Tensor, distance: float = 0.0) -> torch.Tensor:\n",
    "    FieldIn_FT = fft2(FieldIn)\n",
    "    FieldOut_FT = FieldIn_FT*torch.exp(1j*kz*distance)*(torch.imag(kz)==0)\n",
    "    FieldOut = ifft2(FieldOut_FT)\n",
    "    return FieldOut\n",
    "\n",
    "@singledispatch\n",
    "def fidelity(a: Union[np.ndarray, torch.Tensor], b: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot check fidelity of `a`, `b` for {type(a)}, {type(b)}\")\n",
    "\n",
    "@fidelity.register\n",
    "def _(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.square(np.abs(np.sum(normalize(a).conj() * normalize(b))))\n",
    "\n",
    "@fidelity.register\n",
    "def _(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.square(torch.abs(torch.sum(normalize(a).conj() * normalize(b))))\n",
    "\n",
    "@singledispatch\n",
    "def loc_fidelity(a: Union[np.ndarray, torch.Tensor], channel: Union[np.ndarray, torch.Tensor], b: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot check fidelity of `a`, `b` for {type(a)}, {type(b)}\")\n",
    "\n",
    "@loc_fidelity.register\n",
    "def _(a: np.ndarray, channel: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a*channel\n",
    "    return np.square(np.abs(np.sum(normalize(a).conj() * normalize(b))))\n",
    "\n",
    "@loc_fidelity.register\n",
    "def _(a: torch.Tensor, channel: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    a = a*channel\n",
    "    return torch.square(torch.abs(torch.sum(normalize(a).conj() * normalize(b))))\n",
    "\n",
    "@singledispatch\n",
    "def performance_loc_fidelity(A: Union[np.ndarray, torch.Tensor], channels: Union[np.ndarray, torch.Tensor], B: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot check fidelity of `A`, `B` for {type(A)}, {type(B)}\")\n",
    "\n",
    "@performance_loc_fidelity.register\n",
    "def _(A: np.ndarray, channels: np.ndarray, B: np.ndarray) -> Union[np.ndarray, float]:\n",
    "    A = np.squeeze(A)\n",
    "    B = np.squeeze(B)\n",
    "    CH = np.squeeze(channels)\n",
    "    fid_list = np.zeros((A.shape[0]))\n",
    "    for i in range(0, A.shape[0]):\n",
    "        fid_list[i] = loc_fidelity(A[i,:,:], CH[i,:,:], B[i,:,:])\n",
    "    av_loc_fid = 100*np.sum(fid_list)/A.shape[0]\n",
    "    return av_loc_fid, fid_list\n",
    "\n",
    "@performance_loc_fidelity.register\n",
    "def _(A: torch.Tensor, channels: torch.Tensor, B: torch.Tensor) -> Union[torch.Tensor, float]:\n",
    "    A = torch.squeeze(A)\n",
    "    B = torch.squeeze(B)\n",
    "    CH = torch.squeeze(channels)\n",
    "    fid_list = torch.zeros((A.shape[0]))\n",
    "    for i in range(0, A.shape[0]):\n",
    "        fid_list[i] = loc_fidelity(A[i,:,:], CH[i,:,:], B[i,:,:])\n",
    "    av_loc_fid = 100*torch.sum(fid_list)/A.shape[0]\n",
    "    return av_loc_fid, fid_list\n",
    "\n",
    "@singledispatch\n",
    "def performance_efficiency(A: Union[np.ndarray, torch.Tensor], channels: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot check efficiency of `A` for {type(A)}\")\n",
    "\n",
    "@performance_efficiency.register\n",
    "def _(A: np.ndarray, channels: np.ndarray) -> Union[np.ndarray, float]:\n",
    "    A = np.squeeze(A)\n",
    "    CH = np.squeeze(channels)\n",
    "    eff_list = np.zeros((A.shape[0]))\n",
    "    for i in range(0, A.shape[0]):\n",
    "        eff_list[i] = np.sum(A[i,:,:]*CH[i,:,:])\n",
    "    av_eff = 100*np.sum(eff_list)/A.shape[0]\n",
    "    return av_eff, eff_list\n",
    "\n",
    "@performance_efficiency.register\n",
    "def _(A: torch.Tensor, channels: torch.Tensor) -> Union[torch.Tensor, float]:\n",
    "    A = torch.squeeze(A)\n",
    "    CH = torch.squeeze(channels)\n",
    "    eff_list = torch.zeros((A.shape[0]))\n",
    "    for i in range(0, A.shape[0]):\n",
    "        eff_list[i] = torch.sum(A[i,:,:]*CH[i,:,:])\n",
    "    av_eff = 100*torch.sum(eff_list)/A.shape[0]\n",
    "    return av_eff, eff_list\n",
    "\n",
    "@singledispatch\n",
    "def performance_crosstalk(A: Union[np.ndarray, torch.Tensor], channels: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]:\n",
    "    raise NotImplementedError(f\"Cannot check cross-talk for {type(A)}\")\n",
    "\n",
    "@performance_crosstalk.register\n",
    "def _(A: np.ndarray, channels: np.ndarray) -> Union[np.ndarray, np.ndarray, float]:\n",
    "    A = np.squeeze(A)\n",
    "    CH = np.squeeze(channels)\n",
    "    crs_list = np.zeros((A.shape[0]))\n",
    "    crs_matrix = np.zeros((A.shape[0],A.shape[0]))\n",
    "    for i in range(0, A.shape[0]):\n",
    "        for j in range(0, A.shape[0]):\n",
    "            crs_matrix[i,j] = np.sum(A[j,:,:]*CH[i,:,:])\n",
    "    for i in range(0, A.shape[0]): \n",
    "        crs_list[i] = 1 - (crs_matrix[i,i]/np.sum(crs_matrix[:,i]))\n",
    "    av_crs = 100*np.sum(crs_list)/A.shape[0]\n",
    "    return av_crs, crs_list, crs_matrix\n",
    "\n",
    "@performance_crosstalk.register\n",
    "def _(A: torch.Tensor, channels: torch.Tensor) -> Union[torch.Tensor, torch.Tensor, float]:\n",
    "    A = torch.squeeze(A)\n",
    "    CH = torch.squeeze(channels)\n",
    "    crs_list = torch.zeros((A.shape[0]))\n",
    "    crs_matrix = torch.zeros((A.shape[0],A.shape[0]))\n",
    "    for i in range(0, A.shape[0]):\n",
    "        for j in range(0, A.shape[0]):\n",
    "            crs_matrix[i,j] = torch.sum(A[j,:,:]*CH[i,:,:])\n",
    "    for i in range(0, A.shape[0]): \n",
    "        crs_list[i] = 1 - (crs_matrix[i,i]/torch.sum(crs_matrix[:,i]))\n",
    "    av_crs = 100*torch.sum(crs_list)/A.shape[0]\n",
    "    return av_crs, crs_list, crs_matrix\n",
    "\n",
    "@singledispatch\n",
    "def complim(x: Union[np.ndarray, torch.Tensor]):\n",
    "    raise NotImplementedError(f\"Cannot visualize `x` for type: {type(x)}\")\n",
    "\n",
    "@complim.register\n",
    "def _(x: np.ndarray) -> np.ndarray:\n",
    "    mAx = np.amax(np.abs(x))\n",
    "    M = x/mAx\n",
    "    A = np.abs(M)\n",
    "    P = np.angle(M)\n",
    "    A[A > 1.] = 1.\n",
    "    R = A*(np.cos(P - 2*pi/3)/2+0.5)\n",
    "    G = A*(np.cos(P)/2+0.5)\n",
    "    B = A*(np.cos(P + 2*pi/3)/2+0.5)\n",
    "    C = np.dstack((R, G, B))\n",
    "    plt.imshow(C)\n",
    "    plt.show()\n",
    "\n",
    "@complim.register\n",
    "def _(x: torch.Tensor) -> torch.Tensor:\n",
    "    mAx = torch.amax(torch.abs(x))\n",
    "    M = x/mAx\n",
    "    A = torch.abs(M)\n",
    "    P = torch.angle(M)\n",
    "    A[A > 1.] = 1.\n",
    "    R = A*(torch.cos(P - 2*pi/3)/2+0.5)\n",
    "    G = A*(torch.cos(P)/2+0.5)\n",
    "    B = A*(torch.cos(P + 2*pi/3)/2+0.5)\n",
    "    C = torch.dstack((R, G, B))\n",
    "    plt.imshow(C)\n",
    "    plt.show()\n",
    "\n",
    "@singledispatch\n",
    "def plot_in_GS(x: Union[np.ndarray, torch.Tensor]):\n",
    "    raise NotImplementedError(f\"Cannot visualize `x` for type: {type(x)}\")\n",
    "\n",
    "@plot_in_GS.register\n",
    "def _(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.angle(np.exp(1j*x))\n",
    "    plt.imshow(x, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "@plot_in_GS.register\n",
    "def _(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = torch.angle(torch.exp(1j*x))\n",
    "    plt.imshow(x, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "@singledispatch\n",
    "def complim_subplot2(x: Union[np.ndarray, torch.Tensor]):\n",
    "    raise NotImplementedError(f\"Cannot visualize `x` for type: {type(x)}\")\n",
    "\n",
    "@complim_subplot2.register\n",
    "def _(x: np.ndarray, y: np.ndarray, titles: list) -> np.ndarray:\n",
    "    mAx = np.amax(np.abs(x))\n",
    "    M = x/mAx\n",
    "    A = np.abs(M)\n",
    "    P = np.angle(M)\n",
    "    A[A > 1.] = 1.\n",
    "    R = A*(np.cos(P - 2*pi/3)/2+0.5)\n",
    "    G = A*(np.cos(P)/2+0.5)\n",
    "    B = A*(np.cos(P + 2*pi/3)/2+0.5)\n",
    "    C1 = np.dstack((R, G, B))\n",
    "    mAx = np.amax(np.abs(y))\n",
    "    M = y/mAx\n",
    "    A = np.abs(M)\n",
    "    P = np.angle(M)\n",
    "    A[A > 1.] = 1.\n",
    "    R = A*(np.cos(P - 2*pi/3)/2+0.5)\n",
    "    G = A*(np.cos(P)/2+0.5)\n",
    "    B = A*(np.cos(P + 2*pi/3)/2+0.5)\n",
    "    C2 = np.dstack((R, G, B))\n",
    "    C = [C1, C2]\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    i = 0\n",
    "    for ax, interp in zip(axs, titles):\n",
    "        ax.imshow(C[i])\n",
    "        ax.set_title(interp, fontsize=10)\n",
    "        i = i+1\n",
    "    plt.show()\n",
    "\n",
    "@complim_subplot2.register\n",
    "def _(x: torch.Tensor, y: torch.Tensor, titles: list) -> torch.Tensor:\n",
    "    mAx = torch.amax(torch.abs(x))\n",
    "    M = x/mAx\n",
    "    A = torch.abs(M)\n",
    "    P = torch.angle(M)\n",
    "    A[A > 1.] = 1.\n",
    "    R = A*(torch.cos(P - 2*pi/3)/2+0.5)\n",
    "    G = A*(torch.cos(P)/2+0.5)\n",
    "    B = A*(torch.cos(P + 2*pi/3)/2+0.5)\n",
    "    C1 = torch.dstack((R, G, B))\n",
    "    mAx = torch.amax(torch.abs(y))\n",
    "    M = y/mAx\n",
    "    A = torch.abs(M)\n",
    "    P = torch.angle(M)\n",
    "    A[A > 1.] = 1.\n",
    "    R = A*(torch.cos(P - 2*pi/3)/2+0.5)\n",
    "    G = A*(torch.cos(P)/2+0.5)\n",
    "    B = A*(torch.cos(P + 2*pi/3)/2+0.5)\n",
    "    C2 = torch.dstack((R, G, B))\n",
    "    C = [C1, C2]\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    i = 0\n",
    "    for ax, interp in zip(axs, titles):\n",
    "        ax.imshow(C[i])\n",
    "        ax.set_title(interp, fontsize=10)\n",
    "        i = i+1\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "open(\"utils.py\", \"w\", encoding=\"utf-8\").write(UTILS_CODE)\n",
    "print(\"Wrote utils.py\")\n",
    "\n",
    "# 把 MPLC_CUDA2.py 写入（从仓库拷贝的版本，去掉 argparse 的 CLI 退出副作用以支持 notebook 运行）\n",
    "MPLC_CODE = r\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "DEFAULTS = {\n",
    "    \"n_of_modes\": 10,\n",
    "    \"Planes\": 7,\n",
    "    \"iterations\": 60,   # 默认降低迭代，便于烟雾运行\n",
    "    \"alpha\": 1.0,\n",
    "    \"beta\": 2.0,\n",
    "    \"gamma\": 0.0,\n",
    "    \"first_n_iterations\": 10,\n",
    "    \"delta_theta_1\": 2*math.pi/255,\n",
    "    \"delta_theta_0\": 10*(2*math.pi/255),\n",
    "    \"Nx\": 512,\n",
    "    \"Ny\": 512,\n",
    "    \"pixelSize\": 8e-6,\n",
    "    \"wavelength\": 1.57e-6,\n",
    "    \"d_in\": 20e-3,\n",
    "    \"d\": 2*9.7e-3,\n",
    "    \"d_out\": 15e-3,\n",
    "    \"calc_perf_every_it\": 10,\n",
    "    \"equalize_efficiency\": 1,\n",
    "    \"plot_eff_distribution\": 0,\n",
    "    \"smoothing_switch\": 1,\n",
    "    \"OffsetMultiplier\": 0e-5,\n",
    "    \"plot_results\": 0,\n",
    "    \"do_padded_eval\": 0,\n",
    "}\n",
    "\n",
    "def parse_cfg() -> dict:\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument(\"--n_of_modes\", type=int, default=None)\n",
    "    parser.add_argument(\"--Planes\", type=int, default=None)\n",
    "    parser.add_argument(\"--iterations\", type=int, default=None)\n",
    "    parser.add_argument(\"--first_n_iterations\", type=int, default=None)\n",
    "    parser.add_argument(\"--Nx\", type=int, default=None)\n",
    "    parser.add_argument(\"--Ny\", type=int, default=None)\n",
    "    parser.add_argument(\"--calc_perf_every_it\", type=int, default=None)\n",
    "    parser.add_argument(\"--equalize_efficiency\", type=int, choices=[0,1], default=None)\n",
    "    parser.add_argument(\"--plot_eff_distribution\", type=int, choices=[0,1], default=None)\n",
    "    parser.add_argument(\"--smoothing_switch\", type=int, choices=[0,1], default=None)\n",
    "    parser.add_argument(\"--plot_results\", type=int, choices=[0,1], default=None)\n",
    "    parser.add_argument(\"--do_padded_eval\", type=int, choices=[0,1], default=None)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=None)\n",
    "    parser.add_argument(\"--beta\", type=float, default=None)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=None)\n",
    "    parser.add_argument(\"--delta_theta_1\", type=float, default=None)\n",
    "    parser.add_argument(\"--delta_theta_0\", type=float, default=None)\n",
    "    parser.add_argument(\"--pixelSize\", type=float, default=None)\n",
    "    parser.add_argument(\"--wavelength\", type=float, default=None)\n",
    "    parser.add_argument(\"--d_in\", type=float, default=None)\n",
    "    parser.add_argument(\"--d\", type=float, default=None)\n",
    "    parser.add_argument(\"--d_out\", type=float, default=None)\n",
    "    parser.add_argument(\"--OffsetMultiplier\", type=float, default=None)\n",
    "\n",
    "    try:\n",
    "        args = parser.parse_args([])  # notebook 环境避免解析 sys.argv\n",
    "    except SystemExit:\n",
    "        args = argparse.Namespace()\n",
    "    cfg = DEFAULTS.copy()\n",
    "    for k, v in vars(args).items() if hasattr(args, \"__dict__\") else []:\n",
    "        if v is not None:\n",
    "            cfg[k] = v\n",
    "    return cfg\n",
    "\n",
    "CFG = parse_cfg()\n",
    "(n_of_modes, Planes, iterations,\n",
    " alpha, beta, gamma,\n",
    " first_n_iterations, delta_theta_1, delta_theta_0,\n",
    " Nx, Ny, pixelSize, wavelength,\n",
    " d_in, d, d_out,\n",
    " calc_perf_every_it,\n",
    " equalize_efficiency, plot_eff_distribution, smoothing_switch, OffsetMultiplier) = (\n",
    "     CFG[\"n_of_modes\"], CFG[\"Planes\"], CFG[\"iterations\"],\n",
    "     CFG[\"alpha\"], CFG[\"beta\"], CFG[\"gamma\"],\n",
    "     CFG[\"first_n_iterations\"], CFG[\"delta_theta_1\"], CFG[\"delta_theta_0\"],\n",
    "     CFG[\"Nx\"], CFG[\"Ny\"], CFG[\"pixelSize\"], CFG[\"wavelength\"],\n",
    "     CFG[\"d_in\"], CFG[\"d\"], CFG[\"d_out\"],\n",
    "     CFG[\"calc_perf_every_it\"],\n",
    "     CFG[\"equalize_efficiency\"], CFG[\"plot_eff_distribution\"], CFG[\"smoothing_switch\"], CFG[\"OffsetMultiplier\"])\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[MPLC2] Using device: {DEVICE}\")\n",
    "\n",
    "reprW, reprH = Nx * pixelSize, Ny * pixelSize\n",
    "crs_delta = 0.0001 * calc_perf_every_it\n",
    "maskOffset = OffsetMultiplier * np.sqrt(1e-3 / (Nx * Ny * n_of_modes))\n",
    "\n",
    "nx_m = pixelSize*np.linspace(-(Nx-1)/2, (Nx-1)/2, num=Nx)\n",
    "ny_m = pixelSize*np.linspace(-(Ny-1)/2, (Ny-1)/2, num=Ny)\n",
    "X,Y = np.meshgrid(nx_m,ny_m)\n",
    "X_torch = torch.from_numpy(X).to(DEVICE)\n",
    "Y_torch = torch.from_numpy(Y).to(DEVICE)\n",
    "\n",
    "nx = np.linspace(-(Nx-1)/2, (Nx-1)/2, num=Nx)\n",
    "ny = np.linspace(-(Ny-1)/2, (Ny-1)/2, num=Ny)\n",
    "kx, ky = np.meshgrid(2*np.pi*nx/(Nx*pixelSize),2*np.pi*ny/(Ny*pixelSize))\n",
    "\n",
    "lambda_list = np.array([1.53e-6, 1.55e-6, 1.57e-6, 1.59e-6, 1.61e-6, 1.625e-6], dtype=np.float64)\n",
    "lambda_c = 1.57e-6\n",
    "\n",
    "lp_data = np.load('modes_lp_10.npz')\n",
    "lp_modes = lp_data['profiles']\n",
    "gauss_data = np.load('gauss_5x2_custom.npz')\n",
    "gauss_modes = gauss_data['profiles']\n",
    "\n",
    "L = min(lp_modes.shape[0], gauss_modes.shape[0], len(lambda_list))\n",
    "lambda_list = lambda_list[:L]\n",
    "\n",
    "Speckle_basis = lp_modes[:L, 0:n_of_modes, :, :]\n",
    "Gaussian_basis = gauss_modes[:L, 0:n_of_modes, :, :]\n",
    "Speckle_basis_torch = torch.from_numpy(Speckle_basis).to(torch.cdouble).to(DEVICE)\n",
    "Gaussian_basis_torch = torch.from_numpy(Gaussian_basis).to(torch.cdouble).to(DEVICE)\n",
    "\n",
    "Gaussian_Masks = np.zeros_like(Gaussian_basis, dtype=np.float64)\n",
    "for l in range(L):\n",
    "    for m in range(n_of_modes):\n",
    "        inten = np.abs(Gaussian_basis[l, m, :, :]) ** 2\n",
    "        thr = 0.05 * np.max(inten)\n",
    "        Gaussian_Masks[l, m, :, :] = inten > thr\n",
    "Gaussian_Masks_torch = torch.from_numpy(Gaussian_Masks).to(torch.double).to(DEVICE)\n",
    "\n",
    "if (Nx > 512) or (Ny > 512):\n",
    "    pad_x = int((Nx-512)/2)\n",
    "    pad_y = int((Ny-512)/2)\n",
    "    Speckle_basis_torch = nn.functional.pad(Speckle_basis_torch, (pad_x, Nx-512-pad_x, pad_y, Ny-512-pad_y), mode='constant', value=0.+0.j)\n",
    "    Gaussian_basis_torch = nn.functional.pad(Gaussian_basis_torch, (pad_x, Nx-512-pad_x, pad_y, Ny-512-pad_y), mode='constant', value=0.+0.j)\n",
    "    Gaussian_Masks_torch = nn.functional.pad(Gaussian_Masks_torch, (pad_x, Nx-512-pad_x, pad_y, Ny-512-pad_y), mode='constant', value=0.0)\n",
    "\n",
    "phi_bk = torch.ones((Gaussian_Masks_torch.shape[0], Ny, Nx), dtype=torch.double, device=DEVICE) - torch.sum(Gaussian_Masks_torch, axis = 1)\n",
    "phi_cr = torch.zeros((Gaussian_Masks_torch.shape[0], n_of_modes, Ny, Nx), dtype = torch.double, device=DEVICE)\n",
    "for l in range(Gaussian_Masks_torch.shape[0]):\n",
    "    for i in range(n_of_modes):\n",
    "        phi_cr[l,i,:,:] = torch.sum(Gaussian_Masks_torch[l], axis = 0) - Gaussian_Masks_torch[l,i,:,:]\n",
    "\n",
    "phi = Gaussian_basis_torch\n",
    "\n",
    "Masks = torch.zeros((Planes,Ny,Nx), dtype=torch.double, device=DEVICE)\n",
    "Masks_complex = torch.exp(1j*Masks)\n",
    "\n",
    "L = Gaussian_Masks_torch.shape[0]\n",
    "Modes_in = torch.zeros((L, Planes, n_of_modes, Ny, Nx), dtype = torch.cdouble, device=DEVICE)\n",
    "Modes_out = torch.zeros((L, Planes, n_of_modes, Ny, Nx), dtype = torch.cdouble, device=DEVICE)\n",
    "\n",
    "overlap = torch.zeros((n_of_modes), dtype = torch.cdouble, device=DEVICE)\n",
    "eff_distribution = torch.ones((n_of_modes), dtype = torch.double, device=DEVICE)\n",
    "dFdpsi = torch.zeros((L, Planes, n_of_modes, Ny, Nx), dtype = torch.cdouble, device=DEVICE)\n",
    "crs_array_convergence = torch.zeros((max(1,iterations//max(1,calc_perf_every_it))), dtype = torch.double, device=DEVICE)\n",
    "conv_count = 0\n",
    "\n",
    "kz_torch_list = []\n",
    "for l in range(L):\n",
    "    k_l = (2*np.pi)/lambda_list[l]\n",
    "    kz_l = np.sqrt(k_l**2 - (kx**2 + ky**2))\n",
    "    kz_torch_list.append(torch.from_numpy(kz_l.astype(np.cdouble)).to(DEVICE))\n",
    "    Modes_in[l, 0, :, :, :] = propagate_HK(Speckle_basis_torch[l], kz_torch_list[l], d_in)\n",
    "    Modes_out[l, Planes-1, :, :, :] = propagate_HK(phi[l], kz_torch_list[l], -d_out)\n",
    "\n",
    "for i in range(1, iterations+1):\n",
    "    delta_theta = delta_theta_0 if i < first_n_iterations else delta_theta_1\n",
    "    for mask_ind in range(Planes):\n",
    "        for l in range(L):\n",
    "            scale_l = lambda_c / lambda_list[l]\n",
    "            modes = torch.zeros((n_of_modes, Ny, Nx), dtype = torch.cdouble, device=DEVICE)\n",
    "            for pl in range(Planes-1):\n",
    "                mask_cmplx_l = torch.exp(1j*(Masks[pl, :, :]*scale_l))\n",
    "                modes = Modes_in[l, pl, :, :, :] * mask_cmplx_l\n",
    "                modes = propagate_HK(modes, kz_torch_list[l], d)\n",
    "                Modes_in[l, pl+1, :, :, :] = modes\n",
    "            modes_forw_last_plane = Modes_in[l, Planes-1, :, :, :] * torch.exp(1j*(Masks[Planes-1, :, :]*scale_l))\n",
    "            eout_l = propagate_HK(modes_forw_last_plane, kz_torch_list[l], d_out)\n",
    "            for j in range(n_of_modes):\n",
    "                overlap = torch.sum(torch.squeeze(eout_l[j,:,:]) * torch.conj(torch.squeeze(phi[l,j,:,:])))\n",
    "                a = (phi[l, j, :, :]) * overlap\n",
    "                psi_cr_l = (torch.squeeze(eout_l[j,:,:])) * torch.squeeze(phi_cr[l,j,:,:])\n",
    "                psi_bk_l = (torch.squeeze(eout_l[j,:,:])) * phi_bk[l]\n",
    "                dFdpsi[l, Planes-1, j, :, :] = - alpha*a + (beta*psi_cr_l - gamma*psi_bk_l)*0.5\n",
    "            dFdpsi[l, Planes-1, :, :, :] = propagate_HK(dFdpsi[l, Planes-1, :, :, :], kz_torch_list[l], -d_out)\n",
    "            for pl in range(Planes-1, mask_ind, -1):\n",
    "                mask_cmplx_l = torch.exp(1j*(Masks[pl, :, :]*scale_l))\n",
    "                dFdpsi_prop = dFdpsi[l, pl, :, :, :] * torch.conj(mask_cmplx_l)\n",
    "                dFdpsi_prop = propagate_HK(dFdpsi_prop, kz_torch_list[l], -d)\n",
    "                dFdpsi[l, pl-1, :, :, :] = dFdpsi_prop\n",
    "                phi_prop = Modes_out[l, pl, :, :, :] * torch.conj(mask_cmplx_l)\n",
    "                phi_prop = propagate_HK(phi_prop, kz_torch_list[l], -d)\n",
    "                Modes_out[l, pl-1, :, :, :] = phi_prop\n",
    "        if equalize_efficiency == 1:\n",
    "            total_term = torch.zeros((Ny,Nx), dtype=torch.cdouble, device=DEVICE)\n",
    "            for l in range(L):\n",
    "                scale_l = lambda_c / lambda_list[l]\n",
    "                mask_cmplx_l = torch.exp(1j*(Masks[mask_ind, :, :]*scale_l))\n",
    "                weighted_overlaps = torch.zeros((Ny,Nx), dtype=torch.cdouble, device=DEVICE)\n",
    "                for mode in range(n_of_modes):\n",
    "                    weighted_overlaps = weighted_overlaps + (1/eff_distribution[mode]) * torch.squeeze(Modes_in[l, mask_ind, mode, :, :]) * torch.conj(torch.squeeze(dFdpsi[l, mask_ind, mode, :, :]))\n",
    "                total_term = total_term + mask_cmplx_l * weighted_overlaps\n",
    "            delta_P = delta_theta*torch.sign(torch.imag(total_term))\n",
    "        else:\n",
    "            total_term = torch.zeros((Ny,Nx), dtype=torch.cdouble, device=DEVICE)\n",
    "            for l in range(L):\n",
    "                scale_l = lambda_c / lambda_list[l]\n",
    "                mask_cmplx_l = torch.exp(1j*(Masks[mask_ind, :, :]*scale_l))\n",
    "                overlaps = torch.sum(torch.squeeze(Modes_in[l, mask_ind, :, :, :]) * torch.conj(torch.squeeze(dFdpsi[l, mask_ind, :, :, :])), axis = 0)\n",
    "                total_term = total_term + mask_cmplx_l * overlaps\n",
    "            delta_P = delta_theta*torch.sign(torch.imag(total_term))\n",
    "        if smoothing_switch == 1:\n",
    "            ov_sum = torch.zeros((Ny, Nx), dtype=torch.double, device=DEVICE)\n",
    "            for l in range(L):\n",
    "                ov_sum = ov_sum + torch.abs(torch.sum(torch.squeeze(Modes_in[l, mask_ind, :, :, :]*torch.conj(Modes_out[l, mask_ind, :, :, :])), axis = 0))\n",
    "            ovrlp_in_out = ov_sum / L\n",
    "            mask_cmplx = ovrlp_in_out*torch.exp(1j*(Masks[mask_ind, :, :] + delta_P)) \n",
    "            mask_cmplx = mask_cmplx + 0.0\n",
    "            Masks[mask_ind, :, :] = torch.angle(mask_cmplx)\n",
    "        else:\n",
    "            Masks[mask_ind, :, :] = Masks[mask_ind, :, :] + delta_P\n",
    "        Masks_complex[mask_ind, :, :] = torch.exp(1j*torch.squeeze(Masks[mask_ind, :, :]))\n",
    "\n",
    "    if i % max(1,calc_perf_every_it) == 0:\n",
    "        fids = []\n",
    "        crss = []\n",
    "        effs = []\n",
    "        for l in range(L):\n",
    "            scale_l = lambda_c / lambda_list[l]\n",
    "            for pl in range(Planes-1):\n",
    "                mask_cmplx_l = torch.exp(1j*(Masks[pl, :, :]*scale_l))\n",
    "                modes = Modes_in[l, pl, :, :, :]*mask_cmplx_l\n",
    "                modes = propagate_HK(modes, kz_torch_list[l], d)\n",
    "                Modes_in[l, pl+1, :, :, :] = modes\n",
    "            modes = modes*torch.exp(1j*(Masks[Planes-1, :, :]*scale_l))\n",
    "            eout = propagate_HK(modes, kz_torch_list[l], d_out)\n",
    "            eout_int_only = (torch.abs(eout))**2\n",
    "            fid, _ = performance_loc_fidelity(eout, Gaussian_Masks_torch[l], phi[l]) \n",
    "            crs, _, _ = performance_crosstalk(eout_int_only, Gaussian_Masks_torch[l]) \n",
    "            eff, eff_list = performance_efficiency(eout_int_only, Gaussian_Masks_torch[l])\n",
    "            fids.append(fid); crss.append(crs); effs.append(eff)\n",
    "        fid = torch.stack(fids).mean(); crs = torch.stack(crss).mean(); eff = torch.stack(effs).mean()\n",
    "        print('iteration', i, ': loc. fidelity =', round(fid.detach().cpu().numpy().item(),2), ', crosstalk =', round(crs.detach().cpu().numpy().item(),2), ', efficiency =', round(eff.detach().cpu().numpy().item(),2))\n",
    "\n",
    "print(\"Training loop finished.\")\n",
    "\"\"\"\n",
    "\n",
    "open(\"MPLC_CUDA2.py\", \"w\", encoding=\"utf-8\").write(MPLC_CODE)\n",
    "print(\"Wrote MPLC_CUDA2.py\")\n",
    "\n",
    "print(\"Setup done. You can now run the smoke test below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 烟雾测试运行（快速验证）：迭代较少、默认参数\n",
    "# 需要确保 /content/work 下存在 modes_lp_10.npz 与 gauss_5x2_custom.npz\n",
    "\n",
    "import os\n",
    "assert os.path.exists('modes_lp_10.npz'), '缺少 modes_lp_10.npz'\n",
    "assert os.path.exists('gauss_5x2_custom.npz'), '缺少 gauss_5x2_custom.npz'\n",
    "\n",
    "import importlib\n",
    "import MPLC_CUDA2 as runmod\n",
    "importlib.reload(runmod)\n",
    "\n",
    "# 直接导入即执行（脚本式）。若你想在此处覆盖默认参数，可修改上一个单元格中 DEFAULTS。\n",
    "print('Smoke test done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff095886",
   "metadata": {},
   "source": [
    "## 可选：完整运行与参数\n",
    "\n",
    "- 你可以在“环境与依赖安装”单元格中调整 `DEFAULTS`（如 iterations, n_of_modes, Planes 等）。\n",
    "- 若需要保存可视化结果，会写入 `/content/work/results` 目录。\n",
    "\n",
    "常见问题：\n",
    "- 缺数据文件：请挂载 Drive 或配置 GitHub 原始地址，或手动上传到 `/content/work`。\n",
    "- CUDA 不可用：Colab 免费版 GPU 需在菜单 Runtime -> Change runtime type -> GPU。\n",
    "- 内存不足：降低 `n_of_modes`，减少 `iterations`，或关闭 `plot_results`。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
